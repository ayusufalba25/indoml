{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Classical Time Series Analysis*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Statistical Models for Time Series\n",
    "\n",
    "In this chapter, we study some linear statistical models for time series. These models are related to linear regression but account for the correlations that arise between data points in the same time series. In contrast, the linear regression to cross-sectional data assumes that each data point is independent of the others in the sample.\n",
    "\n",
    "Several models will be discussed:\n",
    "- Autoregressive (AR) models, moving average (MA) models, and autoregressive integrated moving average (ARIMA) models\n",
    "- Vector autoregression (VAR)\n",
    "- Hierarchical models.\n",
    "\n",
    "Apart from understanding the time series, the purpose of creating a model usually is to make forecasts with the model. Once we have the model, making a forecast should usually be trivial.\n",
    "\n",
    "Statistical models have their advantages and disadvantages.\n",
    "\n",
    "Advantages:\n",
    "- These models are simple and transparent, so they can be understood clearly in terms of their parameters.\n",
    "- Because of the simple mathematical expressions that define these models, it is possible to derive their properties of interest in a rigorous statistical way.\n",
    "- You can apply these models to fairly small data sets and still get good results.\n",
    "- These simple models and related modifications perform extremely well, even in comparison to very complicated machine learning models. So you get good performance without the danger of overfitting.\n",
    "- Well-developed automated methodologies for choosing orders of your models and estimating their parameters make it simple to generate these forecasts.\n",
    "\n",
    "Disadvantages:\n",
    "- Because these models are quite simple, they don’t always improve performance when given large data sets. If you are working with extremely large data sets, you may do better with the complex models of machine learning and neural network methodologies.\n",
    "- These statistical models put the focus on point estimates of the mean value of a distribution rather than on the distribution. True, you can derive sample variances and the like as some proxy for uncertainty in your forecasts, but your fundamental model offers only limited ways to express uncertainty relative to all the choices you make in selecting a model.\n",
    "- By definition, these models are not built to handle nonlinear dynamics and will do a poor job describing data where nonlinear relationships are dominant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Autoregressive (AR) Models\n",
    "\n",
    "The AR model uses a regression on past values to predict future values.\n",
    "\n",
    "An AR(p) model is described as follows:\n",
    "\n",
    "$$ y_t  = \\Phi_0 + \\Phi_1 * y_{t-1} + ... + \\Phi_p * y_{t-p} + e_t $$\n",
    "\n",
    "To assess the appropriateness of an AR model for your data, begin by plotting the process and its partial autocorrelation function (PACF). The PACF of an AR process should cut off to zero beyond the order *`p`* of an *`AR(p)`* process, giving a concrete and visual indication of the order of an AR process empirically seen in the data.\n",
    "\n",
    "### Sample Case\n",
    "\n",
    "Analyzing. TODO.\n",
    "\n",
    "Making prediction. TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Moving Average (MA) Models\n",
    "\n",
    "A moving average (MA) model relies on a picture of a process in which the value at each point in time is a function of the recent past value “error” terms, each of which is independent from the others. \n",
    "\n",
    "An MA process of order ***q***, ***MA(q)***, is expressed as:\n",
    "\n",
    "$$ y_t = \\mu + e_t + \\theta_1 * e_{t-1} + \\theta_q * e_{t-q} $$\n",
    "\n",
    "The parameter ***q*** can be chosen by looking at the ACF plot (note: not PACF).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Autoregressive Moving Average (ARMA) Models\n",
    "\n",
    "The Autoregressive Moving Average (ARIMA) model combines AR and MA, recognizing that the same time series can have both underlying AR and MA model dynamics.\n",
    "\n",
    "An ARMA model is expressed as:\n",
    "\n",
    "$$ y_t  = \\Phi_0 + \\sum (\\Phi_i * r_{t-i} ) + e_t - \\sum (\\theta_i * e_{t-i} ) $$\n",
    "\n",
    "An ARMA model is not necessarily unique. Because there can be common factors between the AR and MA portions of the equation it’s possible that an ARMA(p, q) model could actually be reduced to another set of parameters. We need to avoid this sort of degenerate situation. In general, you must choose a parsimonious ARMA model.\n",
    "\n",
    "The stationarity of the ARMA process comes down to the stationarity of its AR component and is controlled by the same characteristic equation that controls whether an AR model is stationary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Autoregressive Integrated Moving Average (ARIMA) Models\n",
    "\n",
    "The difference between an ARMA model and an ARIMA model is that the ARIMA model includes the term integrated, which refers to how many times the modeled time series must be differenced to produce stationarity.\n",
    "\n",
    "The ARIMA model is specified in terms of the parameters (p, d, q). In general, the value of each parameter of an ARIMA(p, d, q) model should be kept as small as possible to avoid unwarranted complexity and overfitting to the sample data. As a not-at-all-universal rule of thumb, you should be quite skeptical of values of d over 2 and values of p and q over 5 or so. Also, you should expect either the p or q term to dominate and the other to be relatively small. These are practitioner notes gathered from analysts and not hard-and-fast mathematical truths.\n",
    "\n",
    "Choosing the parameters for ARIMA model can be done manually or automatically (TODO: how in Python?).\n",
    "\n",
    "ARIMA models continue to deliver near state-of-the-art performance, particularly in cases of small data sets where more sophisticated machine learning or deep learning models are not at their best. However, even ARIMA models pose the danger of overfitting despite their relative simplicity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Vector Autoregression (VAR) Models\n",
    "\n",
    "Vector autoregression (VAR) is a stochastic process model used to capture the linear interdependencies among multiple time series. VAR models generalize the univariate autoregressive model (AR model) by allowing for more than one evolving variable. All variables in a VAR enter the model in the same way: each variable has an equation explaining its evolution based on its own lagged values, the lagged values of the other model variables, and an error term. VAR modeling does not require as much knowledge about the forces influencing a variable as do structural models with simultaneous equations: The only prior knowledge required is a list of variables which can be hypothesized to affect each other intertemporally. [WikipediaVAR]\n",
    "\n",
    "### Definition\n",
    "\n",
    "From [[WikipediaVAR]](https://en.wikipedia.org/wiki/Vector_autoregression):\n",
    "\n",
    "A VAR model describes the evolution of a set of `k` variables (called ***endogenous variables***) over the same sample period (`t` = 1, ..., `T`) as a linear function of only their past values. The variables are collected in a `k`-vector ((`k` × 1)-matrix) _y<sub>t</sub>_ , which has as the `i`<sup> th</sup> element, `y`<sub>_i,t_</sub>, the observation at time `t` of the `i`<sup>th</sup> variable. For example, if the `i`<sup> th</sup> variable is GDP, then `y`<sub>_i,t_</sub> is the value of GDP at time `t`.\n",
    "\n",
    "A _p-th order VAR_, denoted **VAR(_p_)**, is\n",
    "\n",
    "$$ y_t = c + A_1 y_{t-1} + A_2 y_{t-2} + \\cdots + A_p y_{t-p} + e_t, \\, $$\n",
    "\n",
    "where the observation `y`<sub>`t`−i</sub> (i periods back) is called the i-th ***lag*** of `y`, `c` is a `k`-vector of constants (intercepts), `A<sub>i</sub>` is a time-invariant (`k` × `k`)-matrix and `e`<sub>`t`</sub> is a `k`-vector of error terms satisfying\n",
    "\n",
    "1. $ \\mathrm{E}(e_t) = 0\\,$ — every error term has mean zero;\n",
    "2. $ \\mathrm{E}(e_t e_t') = \\Omega\\,$ — the contemporaneous covariance matrix of error terms is Ω (a `k` × `k` positive-semidefinite matrix);\n",
    "3. $\\mathrm{E}(e_t e_{t-k}') = 0\\,$ for any non-zero `k` — there is no correlation across time; in particular, no serial correlation in individual error terms.\n",
    "\n",
    "A `p`th-order VAR is also called a **VAR with *p* lags**. The process of choosing the maximum lag `p` in the VAR model requires special attention because inference is dependent on correctness of the selected lag order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Other Models\n",
    "\n",
    "### 4.6.1 Seasonal ARIMA (SARIMA)\n",
    "\n",
    "A Seasonal ARIMA (SARIMA) model assumes multiplicative seasonality. For this reason, a SARIMA model can be expressed as *ARIMA (p, d, q) × (P, D, Q)m*.\n",
    "\n",
    "### 4.6.2 ARCH, GARCH\n",
    "\n",
    "ARCH stands for “Autoregressive Conditional Heteroskedasticity.” This model is used almost exclusively in the finance industry. This class of models is based on the observation that stock prices do not have constant variance, and that in fact the variance itself seems autoregressive conditional on the earlier variances (for example, high-volatility days on the stock exchange come in clusters). In these models, it is the variance of a process that is modeled as an autoregressive process rather than the process itself.\n",
    "\n",
    "### 4.6.3 Hierarchical Time Series Models\n",
    "\n",
    "TODO.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. State Space Models for Time Series\n",
    "\n",
    "- Kalman filter\n",
    "- Hidden Markov models\n",
    "- Bayesian structural time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Brownlee2019] *Introduction to Time Series Forecasting with Python*, Jason Brownlee, v1.7, 2019\n",
    "- [Nielsen2019]: *Practical Time Series Analysis*, Aileen Nielsen, ISBN: 9781492041658, Oct 2019\n",
    "- [WikipediaTS]: *Time Series*, https://en.wikipedia.org/wiki/Time_series\n",
    "- [WikipediaVAR]: *Vector autoregression*, https://en.wikipedia.org/wiki/Vector_autoregression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python [root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
