{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Introduction to Time Series Analysis with Statistical Methods with Python*\n",
    "\n",
    "# 1. Definitions and Terms\n",
    "## 1.1 Definitions\n",
    "\n",
    "#### Time Series\n",
    "\n",
    "[[WikipediaTS]](#references):\n",
    "> A time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.\n",
    "\n",
    "\n",
    "#### Time Series Analysis\n",
    "\n",
    "[[Nielsen2019]](#references):\n",
    "> Time series analysis is the endeavor of extracting meaningful summary and statistical information from points arranged in chronological order. It is done to diagnose past behavior **as well as to predict future behavior**.\n",
    "\n",
    "Some other authors draw distinction between time series **analysis** and time series **forecasting**. In [[Brownlee2019]](#references):\n",
    "> We have different goals depending on whether we are interested in understanding a dataset or making predictions. Understanding a dataset, called time series analysis, can help to make better predictions, but is not required and can result in a large technical investment in time and expertise not directly aligned with the desired outcome, which is forecasting the future.\n",
    "\n",
    "## 1.2 Terms\n",
    "\n",
    "#### univariate time series\n",
    "\n",
    "There is only one variable measured against time\n",
    "\n",
    "#### multivariate time series\n",
    "\n",
    "Series with multiple variables measured at each timestamp. They are particularly rich for analysis because often the measured variables are interrelated and show temporal dependencies between one another.\n",
    "\n",
    "#### temporal\n",
    "\n",
    "Relating to time\n",
    "\n",
    "#### lookahead\n",
    "\n",
    "The term lookahead is used in time series analysis to denote any knowledge of the future. You shouldn’t have such knowledge when designing, training, or evaluating a model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Finding and Preparing Time Series Data\n",
    "\n",
    "## 2.1 Where to Find Sample Time Series Datasets\n",
    "\n",
    "- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) has about 80 time series datasets.\n",
    "- [UEA & UCR Time Series Classification Repository](http://www.timeseriesclassification.com/) contains hundreds of time series datasets\n",
    "\n",
    "\n",
    "## 2.2 Handling Missing Data\n",
    "\n",
    "Common methods to handle missing data:\n",
    "\n",
    "- ***forward fill***: carry forward the last known value prior to the missing one.\n",
    "- ***backward fill***: propagate values backward\n",
    "- ***imputation***: fill in missing data based on observations about the entire data set or with rolling mean/median.\n",
    "- ***interpolation***: use neighboring data points to estimate the missing value. Interpolation can also be a form of imputation.\n",
    "- ***removal***: not use time periods that have missing data at all, if you can afford to loose some data points.\n",
    "\n",
    "Be aware that some methods such as backward fill, imputation, and interpolation introduce [lookahead](#lookahead) bias.\n",
    "\n",
    "## 2.3 Upsampling and Downsampling\n",
    "\n",
    "Downsampling is subsetting data such that the timestamps occur at a lower frequency than in the original time series. Upsampling is representing data as if it were collected more frequently than was actually the case. \n",
    "\n",
    "Easy with Panda's [resample()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.resample.html).\n",
    "\n",
    "## 2.4 Smoothing\n",
    "\n",
    "May be required, e.g. to eliminate spikes and/or error in measurements. Smoothing is done by (moving) averaging, often with weighted moving average method.\n",
    "\n",
    "## 2.5 Seasonal Data\n",
    "\n",
    "Plot the data with line chart to visualize seasonal data. \n",
    "\n",
    "Seasonality, along with level, trend, and residue, can be decomposed using Python's statsmodels [tsa.seasonal.seasonal_decompose()](http://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.seasonal_decompose.html#statsmodels.tsa.seasonal.seasonal_decompose) function:\n",
    "\n",
    "![seasonal decompose output](http://www.statsmodels.org/stable/_images/version0-6-1.png \"Seasonal decomposition output\")\n",
    "\n",
    "\n",
    "Note on **seasonal vs cyclical**:\n",
    "> **Seasonal time series** are time series in which behaviors recur over a fixed period. **Cyclical time series** also exhibit recurring behaviors, but they have a variable period. A common example is a business cycle, such as the stock market’s boom and bust cycles, which have an uncertain duration.\n",
    "\n",
    "## 2.6 Preventing Lookahead\n",
    "\n",
    "Unfortunately, there isn’t a definitive statistical diagnosis for lookahead. So the solution is to just be vigilant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Time Series Exploratory Data Analysis\n",
    "\n",
    "Exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.\n",
    "\n",
    "General EDA methods that are done to general datasets are also applicable to time series datasets. Familiar techniques such as plotting, taking summary statistics, applying histograms, and using targeted scatter plots can be used to answer common questions about the data, such as:\n",
    "- the columns that are available,\n",
    "- their value ranges, \n",
    "- logical units of measurement that work best\n",
    "- if any of the columns are strongly correlated with one another\n",
    "- what is the overall mean of an interesting variable\n",
    "- what is its variance\n",
    "\n",
    "And also to answer questions specific to time series data, such as:\n",
    "- What is the range of values you see, and do they vary by time period or some other logical unit of analysis\n",
    "- Does the data look consistent and uniformly measured, or does it suggest changes in either measurement or behavior over time?\n",
    "\n",
    "This chapter deals with concepts and methods that are specific to analyzing time series data, such as:\n",
    "- stationarity\n",
    "- window functions\n",
    "- self-correlation\n",
    "- spurious correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Stationarity\n",
    "\n",
    "Generally speaking, a stationary time series is one that has fairly stable statistical properties over time, perticularly with respect to mean and variance. Stationarity is important because we need to know how much we should expect the system's long-term past behavior to reflect its long-term future behavior.\n",
    "\n",
    "There are two types of stationarity:\n",
    "- Weak stationarity requires only that the mean and variance of a process be time invariant.\n",
    "- Strong stationarity requires that the distribution of the random variables output by a process remain the same over time.\n",
    "\n",
    "When discuss stationarity in this article, we are referring to weak stationarity.\n",
    "\n",
    "Stationarity matters in practice because a large number of models assume a stationary process, such as traditional models with known strengths and statistical models. Also a model of a non-stationary time series will vary in its accuracy as the metrics of the time series vary. For example, if a model is estimating the mean of the time series, then the bias and error in the model will vary over time with non-stationary time series, at which point the value of the model becomes questionable.\n",
    "\n",
    "### 3.1.1 Stationarity tests\n",
    "\n",
    "Statistical tests for stationary often come down to the question of whether there is a unit root in the process. A linear time series is nonstationary if there is a unit root, although lack of a unit root does not prove stationarity.\n",
    "\n",
    "The Augmented Dickey-Fuller (ADF) test is the most commonly used metric to asses a time series for stationarity problems. This test's null hypothesis is that a unit root is present in a time series. Another test, called the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test posits a null hypothesis that the time series is stationary.\n",
    "\n",
    "TODO ADF\n",
    "TODO KPSS\n",
    "\n",
    "### 3.1.2 Making time series stationary\n",
    "\n",
    "Often time series can be made stationary enough with a few transformations such as:\n",
    "- log and square root transformation to \"fix\" variance\n",
    "- differencing to remove trend (however if you have to difference more than two or three times to make the time series stationary, probably differencing is not the solution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Making time series normally distributed\n",
    "\n",
    "Another common assumption that forecasting models make is that the data is normally distributed. The **Box Cox transformation** makes non0normally distributed data (skewed data) more normal. However, just because you can transform your data doesn't mean you should (e.g. the scale of distance changes between data points).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Window Functions\n",
    "\n",
    "Window function is a common function that distinct to time series. Moving average is probably the most popular window function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Self-correlation and Autocorrelation\n",
    "\n",
    "Self-correlation idea is that a value in a time series at one given point in time may have a correlation to the value at another point in time. Not that \"self-correlation\" is being used here informally to describe a general idea rather than a technical one.\n",
    "\n",
    "Autocorrelation generalizes self-correlation by not anchoring to a specific point in time (?). From Wikipedia:\n",
    "\n",
    "> Autocorrelation, also known as serial correlation, is the correlation of a signal with a delayed copy of itself as a function of the delay. Informally, it is the similarity between observations as a function of the time lag between them.\n",
    "\n",
    "In other words, autocorrelation is how data points at different points in time are linearly related to one another as a function of their time difference.\n",
    "\n",
    "### 3.4.1 The Autocorrelation Function (ACF)\n",
    "\n",
    "The autocorrelation function (ACF) can be intuitively understood with plotting.\n",
    "\n",
    "TODO ACF.\n",
    "\n",
    "There are few important facts about the ACF:\n",
    "- The ACF of a periodic function has the same periodicity as the original process.\n",
    "- The autocorrelation of the sum of periodic functions is the sum of the autocorrelations of each function separately.\n",
    "- All time series have an autocorrelation of 1 at lag 0.\n",
    "- The autocorrelation of a sample of white noise will have a value of approximately 0 at all lags other than 0.\n",
    "- The ACF is symmetric with respect to negative and positive lags, so only positive lags need to be considered explcitly.\n",
    "- A statistical rule for determining a significant nonzero ACF estimate is given by a \"critical region\" with bounds at +/- 1.96 * sqrt(n). This rule relies on sufficiently large sample size and a finite variance for the process.\n",
    "\n",
    "### 3.4.2 The Partial Autocorrelation Function (PACF)\n",
    "\n",
    "Analyzing the same time series above with PACF:\n",
    "\n",
    "TODO PACF.\n",
    "\n",
    "As we can see the PACF removes the correlation with \"harmonics\" of significantly correlated time lag.\n",
    "\n",
    "### 3.4.3 Sample ACF and PACF of Various Time Series\n",
    "\n",
    "#### White Noise\n",
    "\n",
    "TODO\n",
    "\n",
    "#### Random Walk\n",
    "\n",
    "TODO\n",
    "\n",
    "#### Trend\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Spurious correlations\n",
    "\n",
    "Spurious correlation is when two time series have significant but nonsense correlation, for example correlation between number of people who are drowned by falling into a pool and number of films Nicolas Cage appeared in.\n",
    "\n",
    "There are few factors that contribute to producing spurious correlations:\n",
    "- Trend is a big factor.\n",
    "- Seasonality -- for example, the spurious correlation between hot dog consumption and death by drowning (summer).\n",
    "- Level or slope shifts in data from regime changes over time (producing a dumpbell-like distribution with meaningless high correlation).\n",
    "- Cummulatively summed quantities.\n",
    "\n",
    "### 3.5.1 Cointegration\n",
    "\n",
    "A related concept is called *cointegration*, which refers to a real relationship between two time series. A commonly used example is a drunk pedestrian and their dog. Their individually measured walks might appear random taken alone, but they never stray too far from each other.\n",
    "\n",
    "There will be high correlations in the case of cointegration. The difficulty will be in assessing whether the two processes are cointegrated or whether they have spurious correlations, because in both cases they have high correlations. The important difference is that there need not be any relationship in the case of a spurious correlation, whereas cointegrated time series are strongly related to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Statistical Models for Time Series\n",
    "\n",
    "In this chapter, we study some linear statistical models for time series. These models are related to linear regression but account for the correlations that arise between data points in the same time series. In contrast, the linear regression to cross-sectional data assumes that each data point is independent of the others in the sample.\n",
    "\n",
    "Several models will be discussed:\n",
    "- Autoregressive (AR) models, moving average (MA) models, and autoregressive integrated moving average (ARIMA) models\n",
    "- Vector autoregression (VAR)\n",
    "- Hierarchical models.\n",
    "\n",
    "Apart from understanding the time series, the purpose of creating a model usually is to make forecasts with the model. Once we have the model, making a forecast should usually be trivial.\n",
    "\n",
    "Statistical models have their advantages and disadvantages.\n",
    "\n",
    "Advantages:\n",
    "- These models are simple and transparent, so they can be understood clearly in terms of their parameters.\n",
    "- Because of the simple mathematical expressions that define these models, it is possible to derive their properties of interest in a rigorous statistical way.\n",
    "- You can apply these models to fairly small data sets and still get good results.\n",
    "- These simple models and related modifications perform extremely well, even in comparison to very complicated machine learning models. So you get good performance without the danger of overfitting.\n",
    "- Well-developed automated methodologies for choosing orders of your models and estimating their parameters make it simple to generate these forecasts.\n",
    "\n",
    "Disadvantages:\n",
    "- Because these models are quite simple, they don’t always improve performance when given large data sets. If you are working with extremely large data sets, you may do better with the complex models of machine learning and neural network methodologies.\n",
    "- These statistical models put the focus on point estimates of the mean value of a distribution rather than on the distribution. True, you can derive sample variances and the like as some proxy for uncertainty in your forecasts, but your fundamental model offers only limited ways to express uncertainty relative to all the choices you make in selecting a model.\n",
    "- By definition, these models are not built to handle nonlinear dynamics and will do a poor job describing data where nonlinear relationships are dominant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Autoregressive (AR) Models\n",
    "\n",
    "The AR model uses a regression on past values to predict future values.\n",
    "\n",
    "An AR(p) model is described as follows:\n",
    "\n",
    "$$ y_t  = \\Phi_0 + \\Phi_1 * y_{t-1} + ... + \\Phi_p * y_{t-p} + e_t $$\n",
    "\n",
    "To assess the appropriateness of an AR model for your data, begin by plotting the process and its partial autocorrelation function (PACF). The PACF of an AR process should cut off to zero beyond the order *`p`* of an *`AR(p)`* process, giving a concrete and visual indication of the order of an AR process empirically seen in the data.\n",
    "\n",
    "### Sample Case\n",
    "\n",
    "Analyzing. TODO.\n",
    "\n",
    "Making prediction. TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Moving Average (MA) Models\n",
    "\n",
    "A moving average (MA) model relies on a picture of a process in which the value at each point in time is a function of the recent past value “error” terms, each of which is independent from the others. \n",
    "\n",
    "An MA process of order ***q***, ***MA(q)***, is expressed as:\n",
    "\n",
    "$$ y_t = \\mu + e_t + \\theta_1 * e_{t-1} + \\theta_q * e_{t-q} $$\n",
    "\n",
    "The parameter ***q*** can be chosen by looking at the ACF plot (note: not PACF).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Autoregressive Moving Average (ARMA) Models\n",
    "\n",
    "The Autoregressive Moving Average (ARIMA) model combines AR and MA, recognizing that the same time series can have both underlying AR and MA model dynamics.\n",
    "\n",
    "An ARMA model is expressed as:\n",
    "\n",
    "$$ y_t  = \\Phi_0 + \\sum (\\Phi_i * r_{t-i} ) + e_t - \\sum (\\theta_i * e_{t-i} ) $$\n",
    "\n",
    "An ARMA model is not necessarily unique. Because there can be common factors between the AR and MA portions of the equation it’s possible that an ARMA(p, q) model could actually be reduced to another set of parameters. We need to avoid this sort of degenerate situation. In general, you must choose a parsimonious ARMA model.\n",
    "\n",
    "The stationarity of the ARMA process comes down to the stationarity of its AR component and is controlled by the same characteristic equation that controls whether an AR model is stationary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Autoregressive Integrated Moving Average (ARIMA) Models\n",
    "\n",
    "The difference between an ARMA model and an ARIMA model is that the ARIMA model includes the term integrated, which refers to how many times the modeled time series must be differenced to produce stationarity.\n",
    "\n",
    "The ARIMA model is specified in terms of the parameters (p, d, q). In general, the value of each parameter of an ARIMA(p, d, q) model should be kept as small as possible to avoid unwarranted complexity and overfitting to the sample data. As a not-at-all-universal rule of thumb, you should be quite skeptical of values of d over 2 and values of p and q over 5 or so. Also, you should expect either the p or q term to dominate and the other to be relatively small. These are practitioner notes gathered from analysts and not hard-and-fast mathematical truths.\n",
    "\n",
    "Choosing the parameters for ARIMA model can be done manually or automatically (TODO: how in Python?).\n",
    "\n",
    "ARIMA models continue to deliver near state-of-the-art performance, particularly in cases of small data sets where more sophisticated machine learning or deep learning models are not at their best. However, even ARIMA models pose the danger of overfitting despite their relative simplicity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Vector Autoregression (VAR) Models\n",
    "\n",
    "Vector autoregression (VAR) is a stochastic process model used to capture the linear interdependencies among multiple time series. VAR models generalize the univariate autoregressive model (AR model) by allowing for more than one evolving variable. All variables in a VAR enter the model in the same way: each variable has an equation explaining its evolution based on its own lagged values, the lagged values of the other model variables, and an error term. VAR modeling does not require as much knowledge about the forces influencing a variable as do structural models with simultaneous equations: The only prior knowledge required is a list of variables which can be hypothesized to affect each other intertemporally. [WikipediaVAR]\n",
    "\n",
    "### Definition\n",
    "\n",
    "A VAR model describes the evolution of a set of `k` variables (called ***endogenous variables***) over the same sample period (`t` = 1, ..., `T`) as a linear function of only their past values. The variables are collected in a `k`-vector ((`k` × 1)-matrix) _y<sub>t</sub>_ , which has as the `i`<sup> th</sup> element, `y`<sub>_i,t_</sub>, the observation at time `t` of the `i`<sup>th</sup> variable. For example, if the `i`<sup> th</sup> variable is GDP, then `y`<sub>_i,t_</sub> is the value of GDP at time `t`.\n",
    "\n",
    "A _p-th order VAR_, denoted **VAR(_p_)**, is\n",
    "\n",
    "$$ y_t = c + A_1 y_{t-1} + A_2 y_{t-2} + \\cdots + A_p y_{t-p} + e_t, \\, $$\n",
    "\n",
    "where the observation `y`<sub>`t`−i</sub> (i periods back) is called the i-th ***lag*** of `y`, `c` is a `k`-vector of constants (intercepts), `A<sub>i</sub>` is a time-invariant (`k` × `k`)-matrix and `e`<sub>`t`</sub> is a `k`-vector of error terms satisfying\n",
    "\n",
    "1. $ \\mathrm{E}(e_t) = 0\\,$ — every error term has mean zero;\n",
    "2. $ \\mathrm{E}(e_t e_t') = \\Omega\\,$ — the contemporaneous covariance matrix of error terms is Ω (a `k` × `k` positive-semidefinite matrix);\n",
    "3. $\\mathrm{E}(e_t e_{t-k}') = 0\\,$ for any non-zero `k` — there is no correlation across time; in particular, no serial correlation in individual error terms.\n",
    "\n",
    "A `p`th-order VAR is also called a **VAR with *p* lags**. The process of choosing the maximum lag `p` in the VAR model requires special attention because inference is dependent on correctness of the selected lag order.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Other Models\n",
    "\n",
    "### 4.6.1 Seasonal ARIMA (SARIMA)\n",
    "\n",
    "A Seasonal ARIMA (SARIMA) model assumes multiplicative seasonality. For this reason, a SARIMA model can be expressed as *ARIMA (p, d, q) × (P, D, Q)m*.\n",
    "\n",
    "### 4.6.2 ARCH, GARCH\n",
    "\n",
    "ARCH stands for “Autoregressive Conditional Heteroskedasticity.” This model is used almost exclusively in the finance industry. This class of models is based on the observation that stock prices do not have constant variance, and that in fact the variance itself seems autoregressive conditional on the earlier variances (for example, high-volatility days on the stock exchange come in clusters). In these models, it is the variance of a process that is modeled as an autoregressive process rather than the process itself.\n",
    "\n",
    "### 4.6.3 Hierarchical Time Series Models\n",
    "\n",
    "TODO.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Brownlee2019] *Introduction to Time Series Forecasting with Python*, Jason Brownlee, v1.7, 2019\n",
    "- [Nielsen2019]: *Practical Time Series Analysis*, Aileen Nielsen, ISBN: 9781492041658, Oct 2019\n",
    "- [WikipediaTS]: *Time Series*, https://en.wikipedia.org/wiki/Time_series\n",
    "- [WikipediaVAR]: *Vector autoregression*, https://en.wikipedia.org/wiki/Vector_autoregression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
