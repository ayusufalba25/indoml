{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Introduction to Time Series Analysis*\n",
    "\n",
    "# 1. Definitions and Terms\n",
    "## 1.1 Definitions\n",
    "\n",
    "#### Time Series\n",
    "\n",
    "[[WikipediaTS]](#references):\n",
    "> A time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.\n",
    "\n",
    "\n",
    "#### Time Series Analysis\n",
    "\n",
    "[[Nielsen2019]](#references):\n",
    "> Time series analysis is the endeavor of extracting meaningful summary and statistical information from points arranged in chronological order. It is done to diagnose past behavior **as well as to predict future behavior**.\n",
    "\n",
    "Some other authors draw distinction between time series **analysis** and time series **forecasting**. In [[Brownlee2019]](#references):\n",
    "> We have different goals depending on whether we are interested in understanding a dataset or making predictions. Understanding a dataset, called time series analysis, can help to make better predictions, but is not required and can result in a large technical investment in time and expertise not directly aligned with the desired outcome, which is forecasting the future.\n",
    "\n",
    "## 1.2 Terms\n",
    "\n",
    "#### univariate time series\n",
    "\n",
    "There is only one variable measured against time\n",
    "\n",
    "#### multivariate time series\n",
    "\n",
    "Series with multiple variables measured at each timestamp. They are particularly rich for analysis because often the measured variables are interrelated and show temporal dependencies between one another.\n",
    "\n",
    "#### temporal\n",
    "\n",
    "Relating to time\n",
    "\n",
    "#### lookahead\n",
    "\n",
    "The term lookahead is used in time series analysis to denote any knowledge of the future. You shouldn’t have such knowledge when designing, training, or evaluating a model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Finding and Preparing Time Series Data\n",
    "\n",
    "## 2.1 Where to Find Sample Time Series Datasets\n",
    "\n",
    "- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) has about 80 time series datasets.\n",
    "- [UEA & UCR Time Series Classification Repository](http://www.timeseriesclassification.com/) contains hundreds of time series datasets\n",
    "\n",
    "\n",
    "## 2.2 Handling Missing Data\n",
    "\n",
    "Common methods to handle missing data:\n",
    "\n",
    "- ***forward fill***: carry forward the last known value prior to the missing one.\n",
    "- ***backward fill***: propagate values backward\n",
    "- ***imputation***: fill in missing data based on observations about the entire data set or with rolling mean/median.\n",
    "- ***interpolation***: use neighboring data points to estimate the missing value. Interpolation can also be a form of imputation.\n",
    "- ***removal***: not use time periods that have missing data at all, if you can afford to loose some data points.\n",
    "\n",
    "Be aware that some methods such as backward fill, imputation, and interpolation introduce [lookahead](#lookahead) bias.\n",
    "\n",
    "## 2.3 Upsampling and Downsampling\n",
    "\n",
    "Downsampling is subsetting data such that the timestamps occur at a lower frequency than in the original time series. Upsampling is representing data as if it were collected more frequently than was actually the case. \n",
    "\n",
    "Easy with Panda's [resample()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.resample.html).\n",
    "\n",
    "## 2.4 Smoothing\n",
    "\n",
    "May be required, e.g. to eliminate spikes and/or error in measurements. Smoothing is done by (moving) averaging, often with weighted moving average method.\n",
    "\n",
    "## 2.5 Seasonal Data\n",
    "\n",
    "Plot the data with line chart to visualize seasonal data. \n",
    "\n",
    "Seasonality, along with level, trend, and residue, can be decomposed using Python's statsmodels [tsa.seasonal.seasonal_decompose()](http://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.seasonal_decompose.html#statsmodels.tsa.seasonal.seasonal_decompose) function:\n",
    "\n",
    "![seasonal decompose output](http://www.statsmodels.org/stable/_images/version0-6-1.png \"Seasonal decomposition output\")\n",
    "\n",
    "\n",
    "Note on **seasonal vs cyclical**:\n",
    "> **Seasonal time series** are time series in which behaviors recur over a fixed period. **Cyclical time series** also exhibit recurring behaviors, but they have a variable period. A common example is a business cycle, such as the stock market’s boom and bust cycles, which have an uncertain duration.\n",
    "\n",
    "## 2.6 Preventing Lookahead\n",
    "\n",
    "Unfortunately, there isn’t a definitive statistical diagnosis for lookahead. So the solution is to just be vigilant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Time Series Exploratory Data Analysis\n",
    "\n",
    "Exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.\n",
    "\n",
    "General EDA methods that are done to general datasets are also applicable to time series datasets. Familiar techniques such as plotting, taking summary statistics, applying histograms, and using targeted scatter plots can be used to answer common questions about the data, such as:\n",
    "- the columns that are available,\n",
    "- their value ranges, \n",
    "- logical units of measurement that work best\n",
    "- if any of the columns are strongly correlated with one another\n",
    "- what is the overall mean of an interesting variable\n",
    "- what is its variance\n",
    "\n",
    "And also to answer questions specific to time series data, such as:\n",
    "- What is the range of values you see, and do they vary by time period or some other logical unit of analysis\n",
    "- Does the data look consistent and uniformly measured, or does it suggest changes in either measurement or behavior over time?\n",
    "\n",
    "This chapter deals with concepts and methods that are specific to analyzing time series data, such as:\n",
    "- stationarity\n",
    "- window functions\n",
    "- self-correlation\n",
    "- spurious correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Stationarity\n",
    "\n",
    "Generally speaking, a stationary time series is one that has fairly stable statistical properties over time, perticularly with respect to mean and variance. Stationarity is important because we need to know how much we should expect the system's long-term past behavior to reflect its long-term future behavior.\n",
    "\n",
    "There are two types of stationarity:\n",
    "- Weak stationarity requires only that the mean and variance of a process be time invariant.\n",
    "- Strong stationarity requires that the distribution of the random variables output by a process remain the same over time.\n",
    "\n",
    "When discuss stationarity in this article, we are referring to weak stationarity.\n",
    "\n",
    "Stationarity matters in practice because a large number of models assume a stationary process, such as traditional models with known strengths and statistical models. Also a model of a non-stationary time series will vary in its accuracy as the metrics of the time series vary. For example, if a model is estimating the mean of the time series, then the bias and error in the model will vary over time with non-stationary time series, at which point the value of the model becomes questionable.\n",
    "\n",
    "### 3.1.1 Stationarity tests\n",
    "\n",
    "Statistical tests for stationary often come down to the question of whether there is a unit root in the process. A linear time series is nonstationary if there is a unit root, although lack of a unit root does not prove stationarity.\n",
    "\n",
    "The Augmented Dickey-Fuller (ADF) test is the most commonly used metric to asses a time series for stationarity problems. This test's null hypothesis is that a unit root is present in a time series. Another test, called the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test posits a null hypothesis that the time series is stationary.\n",
    "\n",
    "TODO ADF\n",
    "TODO KPSS\n",
    "\n",
    "### 3.1.2 Making time series stationary\n",
    "\n",
    "Often time series can be made stationary enough with a few transformations such as:\n",
    "- log and square root transformation to \"fix\" variance\n",
    "- differencing to remove trend (however if you have to difference more than two or three times to make the time series stationary, probably differencing is not the solution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Making time series normally distributed\n",
    "\n",
    "Another common assumption that forecasting models make is that the data is normally distributed. The **Box Cox transformation** makes non0normally distributed data (skewed data) more normal. However, just because you can transform your data doesn't mean you should (e.g. the scale of distance changes between data points).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Window Functions\n",
    "\n",
    "Window function is a common function that distinct to time series. Moving average is probably the most popular window function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Self-correlation and Autocorrelation\n",
    "\n",
    "Self-correlation idea is that a value in a time series at one given point in time may have a correlation to the value at another point in time. Not that \"self-correlation\" is being used here informally to describe a general idea rather than a technical one.\n",
    "\n",
    "Autocorrelation generalizes self-correlation by not anchoring to a specific point in time (?). From Wikipedia:\n",
    "\n",
    "> Autocorrelation, also known as serial correlation, is the correlation of a signal with a delayed copy of itself as a function of the delay. Informally, it is the similarity between observations as a function of the time lag between them.\n",
    "\n",
    "In other words, autocorrelation is how data points at different points in time are linearly related to one another as a function of their time difference.\n",
    "\n",
    "### 3.4.1 The Autocorrelation Function (ACF)\n",
    "\n",
    "The autocorrelation function (ACF) can be intuitively understood with plotting.\n",
    "\n",
    "TODO ACF.\n",
    "\n",
    "There are few important facts about the ACF:\n",
    "- The ACF of a periodic function has the same periodicity as the original process.\n",
    "- The autocorrelation of the sum of periodic functions is the sum of the autocorrelations of each function separately.\n",
    "- All time series have an autocorrelation of 1 at lag 0.\n",
    "- The autocorrelation of a sample of white noise will have a value of approximately 0 at all lags other than 0.\n",
    "- The ACF is symmetric with respect to negative and positive lags, so only positive lags need to be considered explcitly.\n",
    "- A statistical rule for determining a significant nonzero ACF estimate is given by a \"critical region\" with bounds at +/- 1.96 * sqrt(n). This rule relies on sufficiently large sample size and a finite variance for the process.\n",
    "\n",
    "### 3.4.2 The Partial Autocorrelation Function (PACF)\n",
    "\n",
    "Analyzing the same time series above with PACF:\n",
    "\n",
    "TODO PACF.\n",
    "\n",
    "As we can see the PACF removes the correlation with \"harmonics\" of significantly correlated time lag.\n",
    "\n",
    "### 3.4.3 Sample ACF and PACF of Various Time Series\n",
    "\n",
    "#### White Noise\n",
    "\n",
    "TODO\n",
    "\n",
    "#### Random Walk\n",
    "\n",
    "TODO\n",
    "\n",
    "#### Trend\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Spurious correlations\n",
    "\n",
    "Spurious correlation is when two time series have significant but nonsense correlation, for example correlation between number of people who are drowned by falling into a pool and number of films Nicolas Cage appeared in.\n",
    "\n",
    "There are few factors that contribute to producing spurious correlations:\n",
    "- Trend is a big factor.\n",
    "- Seasonality -- for example, the spurious correlation between hot dog consumption and death by drowning (summer).\n",
    "- Level or slope shifts in data from regime changes over time (producing a dumpbell-like distribution with meaningless high correlation).\n",
    "- Cummulatively summed quantities.\n",
    "\n",
    "### 3.5.1 Cointegration\n",
    "\n",
    "A related concept is called *cointegration*, which refers to a real relationship between two time series. A commonly used example is a drunk pedestrian and their dog. Their individually measured walks might appear random taken alone, but they never stray too far from each other.\n",
    "\n",
    "There will be high correlations in the case of cointegration. The difficulty will be in assessing whether the two processes are cointegrated or whether they have spurious correlations, because in both cases they have high correlations. The important difference is that there need not be any relationship in the case of a spurious correlation, whereas cointegrated time series are strongly related to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Brownlee2019] *Introduction to Time Series Forecasting with Python*, Jason Brownlee, v1.7, 2019\n",
    "- [Nielsen2019]: *Practical Time Series Analysis*, Aileen Nielsen, ISBN: 9781492041658, Oct 2019\n",
    "- [WikipediaTS]: *Time Series*, https://en.wikipedia.org/wiki/Time_series\n",
    "- [WikipediaVAR]: *Vector autoregression*, https://en.wikipedia.org/wiki/Vector_autoregression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python [root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
